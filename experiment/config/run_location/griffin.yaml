# @package _global_

dsettings:
  trainer:
    cuda: TRUE
  storage:
    logs_dir: 'logs'
    data_root: '${oc.env:HOME}/workspace/research/s12045/data/dataset'
  dataset:
    gpu_augment: FALSE
    prepare: TRUE
    try_in_memory: TRUE

trainer:
  prepare_data_per_node: TRUE

dataloader:
  num_workers: 32  # max 128, more than 16 doesn't really seem to help (tested on batch size 256*3)?
  pin_memory: ${dsettings.trainer.cuda}
  batch_size: ${settings.dataset.batch_size}

hydra:
  job:
    name: 's12045'
  run:
    dir: '${dsettings.storage.logs_dir}/hydra_run/${now:%Y-%m-%d_%H-%M-%S}_${hydra.job.name}'
  sweep:
    dir: '${dsettings.storage.logs_dir}/hydra_sweep/${now:%Y-%m-%d_%H-%M-%S}_${hydra.job.name}'
    subdir: '${hydra.job.id}' # hydra.job.id is not available for dir
